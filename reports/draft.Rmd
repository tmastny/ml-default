---
title: "Final Project Draft"
author: "Tim Mastny"
date: "April 17, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
knitr::opts_knit$set(root.dir = here::here())
```

# Credit Card Defaults

This project is to predict credit card defaults. The following [study](https://github.com/wangzongyan/Default-of-credit-card-clients-Data-Project/blob/master/The%20comparisons%20of%20data%20mining%20techniques%20for%20the%20predictive%20accuracy%20of%20probability%20of%20default%20of%20credit%20card%20clients.pdf) was conducted on the [dataset](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients):

> Yeh, I. C., & Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2), 2473-2480

The study found the following error rates based on a validation set. My goal is to see if we can replicate and improve on these results. 

```{r echo=FALSE, out.width="500px", fig.align='center'}
knitr::include_graphics("../paper-error-rates.png")
```

## Cleaning

### Formatting as .csv

The first thing we need to do is to create a `.csv` out of the original `.xls` file type found on UCI Machine Learning [Repository](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients). 

The file is formatted with two headers, so we'll be sure to skip the first one:

```{r echo=FALSE, message=FALSE}
library(here)
knitr::read_chunk(here("R", "to_csv.R"))
```
```{r to_csv, eval=FALSE}
```

This makes it more portable between systems, especially if we want to read it into Python to use Keras for deep learning. 

### Tidying

Let's take a look at our data:

```{r echo=FALSE}
knitr::read_chunk(here("R", "cleaner.R"))
```
```{r cleaner1}
```

```{r}
glimpse(d)
d %>% 
  select_if(~any(is.na(.)))
```

The data looks pretty clean. All numerics without any NAs. 

Next, we'll split the data into a training and test set, since that is how the paper evaluated their models. The paper doesn't specify the split percent, so we will use 33%. Even though the data is unbalanced:

```{r}
d %>%
  group_by(default) %>%
  summarise(count = n())
```

The paper explicitly says it created the validation set randomly, so we will avoid using stratified sampling. 

```{r cleaner2, eval=FALSE}
```

Next, we'll use the `recipes` package to process our data. Many of the columns in the dataset are actually categorical variables encoded as integers. To be consistent between Python and R methods which may or may not accept categorical features, I will use recipes to dummy encode as necessary.

```{r cleaner3, eval=FALSE}
```

Lastly, we'll also standardize the data set, a standard technique used in some of the study's methods such as kNN and neural networks. The study does not say whether or not this sort of data pre-processing was used, so we will include it in our testing to see whether it impacts our results.

Also important: we'll use the training data to calculate the mean and standard deviation, so our testing data doesn't leak into our training.

```{r cleaner4, eval=FALSE}
```

Then we will save the data as a `.csv` for future training.

```{r cleaner5, eval=FALSE}
```

```{r}
glimpse(read_csv(here("data", "train.csv")))
```




